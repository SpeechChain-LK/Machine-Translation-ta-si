{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd86f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with GPU support (CUDA 11.8 version)\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())  # Should be True\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))  # Should print Quadro RTX 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ffaba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch datasets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ac179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc9a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8663fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6542781",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.src_lang = \"ta_IN\"\n",
    "tokenizer.tgt_lang = \"si_LK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833dd991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"train.tsv\",\n",
    "        \"validation\": \"val.tsv\",\n",
    "        \"test\": \"test.tsv\"\n",
    "    },\n",
    "    delimiter=\"\\t\",  # TSV format\n",
    "    column_names=[\"source\", \"target\"]  # Only needed if your files don't have headers\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "print(\"Train Sample:\", dataset[\"train\"][1])\n",
    "print(\"Validation Sample:\", dataset[\"validation\"][1])\n",
    "print(\"Test Sample:\", dataset[\"test\"][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5abbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip uninstall tensorflow -y\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f5388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples[\"source\"]]  # Source: Tamil\n",
    "    targets = [ex for ex in examples[\"target\"]]  # Target: Sinhala\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "   \n",
    "    # Tokenize target language\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "   \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e36dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"transformers[torch]\"\n",
    "%pip install -U \"accelerate>=0.26.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff40765",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sacrebleu\n",
    "%pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb2608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import os\n",
    "\n",
    "class SavePerEpochCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        epoch_dir = os.path.join(args.output_dir, f\"epoch_{int(state.epoch)}_model\")\n",
    "        os.makedirs(epoch_dir, exist_ok=True)\n",
    "        kwargs[\"model\"].save_pretrained(epoch_dir)\n",
    "        self.tokenizer.save_pretrained(epoch_dir)\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install rouge_score\n",
    "! pip install bert_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02dca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "bleu = load(\"sacrebleu\")\n",
    "rouge = load(\"rouge\")\n",
    "chrf = load(\"chrf\")\n",
    "bart_score = load(\"bertscore\")  # No direct \"bartscore\", use bertscore or integrate external lib\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "\n",
    "    bleu_result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    rouge_result = rouge.compute(predictions=decoded_preds, references=[l[0] for l in decoded_labels])\n",
    "    chrf_result = chrf.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bertscore_result = bart_score.compute(predictions=decoded_preds, references=[l[0] for l in decoded_labels], lang=\"si\")\n",
    "\n",
    "\n",
    "    # Exact Match\n",
    "    em = np.mean([p == l[0] for p, l in zip(decoded_preds, decoded_labels)])\n",
    "\n",
    "\n",
    "    # Token Accuracy\n",
    "    total = correct = 0\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        pred_tokens = pred.split()\n",
    "        label_tokens = label[0].split()\n",
    "        total += len(label_tokens)\n",
    "        correct += sum([p == l for p, l in zip(pred_tokens, label_tokens)])\n",
    "    token_acc = correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_result[\"score\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "        \"chrf\": chrf_result[\"score\"],\n",
    "        \"exact_match\": em,\n",
    "        \"token_accuracy\": token_acc,\n",
    "        \"bertscore_f1\": np.mean(bertscore_result[\"f1\"])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ad30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers[torch]\n",
    "%pip install \"accelerate>=0.26.0\"\n",
    "%pip install tensorboard\n",
    "%pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0caa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"accelerate>=0.26.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47bf389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results_ta_si\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",       \n",
    "    save_steps=3,              \n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,  # True if you have GPU\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, DataCollatorForSeq2Seq\n",
    "\n",
    "#  Data collator for padding and batching\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[SavePerEpochCallback(tokenizer)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac59bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=\"2.results_ta_si/checkpoint-5418\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae00f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mbart50-env-0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
