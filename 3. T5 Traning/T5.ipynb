{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12187247,"sourceType":"datasetVersion","datasetId":7676340},{"sourceId":12187615,"sourceType":"datasetVersion","datasetId":7676609}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install transformers torch datasets sentencepiece\n%pip install protobuf\n%pip install -U \"transformers[torch]\"\n%pip install -U \"accelerate>=0.26.0\"\n%pip install sacrebleu\n%pip install evaluate\n%pip install sacrebleu\n%pip install evaluate\n%pip install rouge_score\n%pip install bert_score\n%pip install tensorboard\n%pip install accelerate>=0.26.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T03:37:53.478155Z","iopub.execute_input":"2025-06-17T03:37:53.478384Z","execution_failed":"2025-06-17T03:44:28.770Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dc9260cda10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cudnn-cu12/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dc92587db90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cudnn-cu12/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dc92587e510>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cudnn-cu12/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dc92587f050>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cudnn-cu12/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dc92587f910>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cudnn-cu12/\u001b[0m\u001b[33m\n\u001b[0mINFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dc925871cd0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dc925871090>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dc92586cb50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dc925a5c350>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dc92588c3d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch) (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\"\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (3.20.3)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.51.3)\n\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c086b9d3890>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/transformers/\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# # Load model and tokenizer\n# model_name = \"t5-small\"\n# tokenizer = T5Tokenizer.from_pretrained(model_name)\n# model = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# tokenizer.src_lang = \"ta_IN\"\n# tokenizer.tgt_lang = \"si_LK\"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-17T03:44:28.771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model Architecture Tweaks\n\n# Load model and tokenizer\nfrom transformers import T5ForConditionalGeneration, T5Config\n\n# Try a larger model if possible\nmodel_name = \"t5-small\"  # or \"t5-large\" if you have resources\ntokenizer = T5Tokenizer.from_pretrained(model_name)\n\n# Or customize config\nconfig = T5Config.from_pretrained(\n    model_name,\n    dropout_rate=0.1,\n    layer_norm_epsilon=1e-6,\n    d_ff=2048  # Larger feed-forward layers\n)\n\nmodel = T5ForConditionalGeneration.from_pretrained(model_name, config=config)\n\ntokenizer.src_lang = \"ta_IN\"\ntokenizer.tgt_lang = \"si_LK\"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-16T19:11:55.526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\n    \"csv\",\n    data_files={\n        \"train\": \"/kaggle/input/t5-model-03/train.tsv\",\n        \"validation\": \"/kaggle/input/t5-model-03/val.tsv\",\n        \"test\": \"/kaggle/input/t5-model-03/test.tsv\"\n    },\n    delimiter=\"\\t\",  # TSV format\n    column_names=[\"source\", \"target\"]  # Only needed if your files don't have headers\n)\n\n# Example usage\nprint(\"Train Sample:\", dataset[\"train\"][1])\nprint(\"Validation Sample:\", dataset[\"validation\"][1])\nprint(\"Test Sample:\", dataset[\"test\"][1])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-16T19:11:55.527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = dataset.filter(\n    lambda x: x[\"source\"].strip() != \"\" and x[\"target\"].strip() != \"\"\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-16T19:11:55.528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for empty source/target strings\nempty_source = [ex for ex in dataset[\"train\"] if not ex[\"source\"].strip()]\nempty_target = [ex for ex in dataset[\"train\"] if not ex[\"target\"].strip()]\n\nprint(f\"Empty source samples: {len(empty_source)}\")\nprint(f\"Empty target samples: {len(empty_target)}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-16T19:11:55.528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_empty_values(split):\n    print(f\"\\nAnalyzing {split} split:\")\n    empty_count = 0\n    whitespace_count = 0\n    \n    for i, example in enumerate(dataset[split]):\n        source_empty = not example[\"source\"].strip()\n        target_empty = not example[\"target\"].strip()\n        \n        if source_empty or target_empty:\n            empty_count += 1\n            print(f\"Index {i}:\")\n            print(f\"  Source: {'EMPTY' if source_empty else repr(example['source'])}\")\n            print(f\"  Target: {'EMPTY' if target_empty else repr(example['target'])}\")\n            \n        if any(c.isspace() for c in example[\"source\"]) or any(c.isspace() for c in example[\"target\"]):\n            whitespace_count += 1\n    \n    print(f\"\\nTotal empty examples: {empty_count}\")\n    print(f\"Examples with only whitespace: {whitespace_count}\")\n\n# Run analysis on all splits\nfor split in [\"train\", \"validation\", \"test\"]:\n    analyze_empty_values(split)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-16T19:11:55.529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndef check_with_pandas(split):\n    df = pd.DataFrame(dataset[split])\n    \n    # Find empty strings\n    empty_src = df[df[\"source\"].str.strip() == \"\"]\n    empty_tgt = df[df[\"target\"].str.strip() == \"\"]\n    \n    print(f\"\\n{split} split empty sources: {len(empty_src)}\")\n    print(f\"{split} split empty targets: {len(empty_tgt)}\")\n    \n    # Show samples with empty values\n    if not empty_src.empty:\n        print(\"\\nEmpty source examples:\")\n        print(empty_src.head())\n    if not empty_tgt.empty:\n        print(\"\\nEmpty target examples:\")\n        print(empty_tgt.head())\n\nfor split in [\"train\", \"validation\", \"test\"]:\n    check_with_pandas(split)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-16T19:11:55.529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef visualize_empty_values():\n    splits = [\"train\", \"validation\", \"test\"]\n    empty_counts = {}\n    \n    for split in splits:\n        empty_src = sum(1 for ex in dataset[split] if not ex[\"source\"].strip())\n        empty_tgt = sum(1 for ex in dataset[split] if not ex[\"target\"].strip())\n        empty_counts[split] = (empty_src, empty_tgt)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    x = range(len(splits))\n    width = 0.35\n    \n    ax.bar(x, [v[0] for v in empty_counts.values()], width, label='Empty Sources')\n    ax.bar([p + width for p in x], [v[1] for v in empty_counts.values()], width, label='Empty Targets')\n    \n    ax.set_ylabel('Count')\n    ax.set_title('Empty Values by Dataset Split')\n    ax.set_xticks([p + width/2 for p in x])\n    ax.set_xticklabels(splits)\n    ax.legend()\n    \n    plt.show()\n\nvisualize_empty_values()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-16T19:11:55.529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_dataset(examples):\n    return {\n        \"source\": [s.strip() for s in examples[\"source\"]],\n        \"target\": [t.strip() for t in examples[\"target\"]]\n    }\n\ndataset = dataset.map(clean_dataset, batched=True)\ndataset = dataset.filter(\n    lambda x: len(x[\"source\"]) > 0 and len(x[\"target\"]) > 0\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-16T19:11:55.530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_function(examples):\n    # Add prefix for T5 (important!)\n    inputs = [\"translate Tamil to Sinhala: \" + ex for ex in examples[\"source\"]]\n    targets = [ex for ex in examples[\"target\"]]\n    \n    model_inputs = tokenizer(\n        inputs, \n        max_length=128, \n        truncation=True,\n        padding=\"max_length\"  # Ensure consistent length\n    )\n   \n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets, \n            max_length=128, \n            truncation=True,\n            padding=\"max_length\"\n        )\n    \n    # Replace padding token id with -100 for loss calculation\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n        for label in labels[\"input_ids\"]\n    ]\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-16T19:11:55.530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainerCallback\nimport os\n\nclass SavePerEpochCallback(TrainerCallback):\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def on_epoch_end(self, args, state, control, **kwargs):\n        epoch_dir = os.path.join(args.output_dir, f\"epoch_{int(state.epoch)}_model\")\n        os.makedirs(epoch_dir, exist_ok=True)\n        kwargs[\"model\"].save_pretrained(epoch_dir)\n        self.tokenizer.save_pretrained(epoch_dir)\n        return control\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from evaluate import load\nimport numpy as np\nimport torch\n\nbleu = load(\"sacrebleu\")\nrouge = load(\"rouge\")\nchrf = load(\"chrf\")\nbart_score = load(\"bertscore\")  # No direct \"bartscore\", use bertscore or integrate external lib\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n    \ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n# In your compute_metrics function:\n\n    decoded_preds = [pred if pred.strip() != \"\" else \"[EMPTY]\" for pred in decoded_preds]\n    decoded_labels = [label if label.strip() != \"\" else \"[EMPTY]\" for label in decoded_labels]\n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    bleu_result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n    rouge_result = rouge.compute(predictions=decoded_preds, references=[l[0] for l in decoded_labels])\n    chrf_result = chrf.compute(predictions=decoded_preds, references=decoded_labels)\n    bertscore_result = bart_score.compute(predictions=decoded_preds, references=[l[0] for l in decoded_labels], lang=\"si\")\n\n\n    # Exact Match\n    em = np.mean([p == l[0] for p, l in zip(decoded_preds, decoded_labels)])\n\n\n    # Token Accuracy\n    total = correct = 0\n    for pred, label in zip(decoded_preds, decoded_labels):\n        pred_tokens = pred.split()\n        label_tokens = label[0].split()\n        total += len(label_tokens)\n        correct += sum([p == l for p, l in zip(pred_tokens, label_tokens)])\n    token_acc = correct / total if total > 0 else 0\n\n\n    return {\n        \"bleu\": bleu_result[\"score\"],\n        \"rougeL\": rouge_result[\"rougeL\"],\n        \"chrf\": chrf_result[\"score\"],\n        \"exact_match\": em,\n        \"token_accuracy\": token_acc,\n        \"bertscore_f1\": np.mean(bertscore_result[\"f1\"])\n    }\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-16T19:11:55.530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./kaggle/working/T5-results_ta_si\",\n    evaluation_strategy=\"steps\",\n    eval_steps=100,  # More frequent evaluation\n    save_strategy=\"steps\",\n    save_steps=10,\n    save_total_limit=3,\n    logging_dir=\"./kaggle/working/logs\",\n    logging_steps=10,\n    learning_rate=3e-4,  # T5 typically uses higher LR\n    per_device_train_batch_size=16,  # Reduce if OOM occurs\n    per_device_eval_batch_size=16,\n    weight_decay=0.01,\n    num_train_epochs=30,\n    predict_with_generate=True,\n    fp16=True,\n    warmup_steps=1000,\n    gradient_accumulation_steps=4,  # Effective larger batch size\n    optim=\"adafactor\",  # T5's recommended optimizer\n    report_to=\"tensorboard\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"bleu\",\n    greater_is_better=True\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-16T19:11:55.530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n\n#  Data collator for padding and batching\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    padding=True,\n    max_length=128,\n    return_tensors=\"pt\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[SavePerEpochCallback(tokenizer)]\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-16T19:11:55.530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.tran()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-16T19:11:55.525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trainer.train(resume_from_checkpoint=\"/kaggle/working/kaggle/working/T5-results_ta_si/checkpoint-10280\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-16T19:11:55.525Z"}},"outputs":[],"execution_count":null}]}